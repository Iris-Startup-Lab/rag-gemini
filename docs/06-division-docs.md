Buen√≠sima pregunta, aqu√≠ est√° justo el ‚Äúc√≥mo se llena de PDFs‚Äù este backend üíæüìö

Te respondo en tres partes:

1. **Concepto general (leyes / tr√°mites / general)**
2. **Flujo concreto paso a paso**
3. **Script para cargar carpetas locales (leyes y tr√°mites)**

---

## 1Ô∏è‚É£ Concepto: c√≥mo mapeamos leyes / tr√°mites / general

Con el c√≥digo que ya tenemos, **cada ‚Äúcategor√≠a‚Äù es un File Search Store de Gemini**:

* Un store para **leyes**
* Un store para **tr√°mites**
* (Opcional pero s√∫per √∫til) un store **general** que tenga *ambas cosas* para b√∫squedas globales.

Entonces:

* Si en el frontend seleccionan **‚ÄúSolo leyes‚Äù** ‚Üí llamas al backend con el `store_name` de leyes.
* Si seleccionan **‚ÄúSolo tr√°mites‚Äù** ‚Üí usas el `store_name` de tr√°mites.
* Si seleccionan **‚ÄúGeneral‚Äù** ‚Üí usas el `store_name` del store general (donde subimos leyes + tr√°mites).

üëÄ S√≠, esto implica duplicar los documentos en el `store_general`, pero con ~170 archivos es totalmente manejable para un MVP y mantiene el backend simple.

---

## 2Ô∏è‚É£ Paso a paso: antes / despu√©s de `uvicorn main:app --reload`

En realidad, **necesitas tener el servidor levantado** para poder usar los endpoints de carga, as√≠ que el orden sano es:

### Paso 0 ‚Äî Estructura de carpetas local

En tu repo, crea algo as√≠:

```bash
data/
  leyes/
    (aqu√≠ pones tus ~150 PDFs de leyes)
  tramites/
    (aqu√≠ pones tus ~20 PDFs de tr√°mites)
```

No hace falta que el backend ‚Äúvea‚Äù estas carpetas directamente: las vamos a usar desde un script que llama al endpoint de carga.

---

### Paso 1 ‚Äî Levantar el backend

```bash
uvicorn main:app --reload
```

Backend escuchando en `http://localhost:8000`.

---

### Paso 2 ‚Äî Crear los 3 stores en Gemini v√≠a API

Ve a `http://localhost:8000/docs` (Swagger) y:

1. Abre `POST /create-store`.

2. En ‚ÄúRequest body‚Äù, pon algo como:

   * Para **leyes**:

     ```json
     {
       "display_name": "leyes"
     }
     ```
   * Para **tr√°mites**:

     ```json
     {
       "display_name": "tramites"
     }
     ```
   * Para **general**:

     ```json
     {
       "display_name": "general"
     }
     ```

3. Ejecuta cada uno y **guarda los `store_name`** que te regrese la API (son IDs largos tipo `projects/xxx/locations/xxx/fileStores/yyy`).

Te sugiero apuntarlos en tu `.env` para tenerlos a la mano:

```env
GEMINI_STORE_LEYES=projects/.../stores/...
GEMINI_STORE_TRAMITES=projects/.../stores/...
GEMINI_STORE_GENERAL=projects/.../stores/...
```

---

### Paso 3 ‚Äî Cargar tus PDFs desde carpetas locales

Aqu√≠ es donde entra la parte de ‚Äú¬øse puede tener una ruta espec√≠fica para que los tome?‚Äù.

Vamos a a√±adir un **script peque√±o** que:

* Lee una carpeta local (`data/leyes` o `data/tramites`).
* Env√≠a TODOS los archivos al endpoint `POST /upload-files/{store_name}`.

De esta forma t√∫ solo dejas los PDFs en la carpeta y ejecutas un comando.

---

## 3Ô∏è‚É£ Script: `scripts/batch_upload.py`

Crea la carpeta `scripts/` y dentro el archivo `batch_upload.py`:

```python
# scripts/batch_upload.py
import argparse
import os
from pathlib import Path
import mimetypes

import requests


API_BASE = "http://localhost:8000"


def collect_files(folder: Path):
    """
    Recorre la carpeta y prepara la lista de (campo, (filename, fileobj, mimetype))
    para el multipart/form-data que espera FastAPI.
    """
    files_payload = []

    for entry in sorted(folder.iterdir()):
        if not entry.is_file():
            continue

        mime, _ = mimetypes.guess_type(entry.name)
        if mime is None:
            # Por defecto asumimos PDF si no se puede adivinar
            mime = "application/pdf"

        fileobj = open(entry, "rb")
        files_payload.append(
            (
                "files",  # debe coincidir con el par√°metro 'files' del endpoint
                (entry.name, fileobj, mime),
            )
        )

    return files_payload


def main():
    parser = argparse.ArgumentParser(
        description="Sube en batch archivos de una carpeta a un File Search store."
    )
    parser.add_argument(
        "--store-name",
        required=True,
        help="Nombre completo del File Search store (store_name devuelto por /create-store).",
    )
    parser.add_argument(
        "--folder",
        required=True,
        help="Ruta a la carpeta local con los archivos (ej. data/leyes).",
    )

    args = parser.parse_args()

    folder = Path(args.folder)
    if not folder.exists() or not folder.is_dir():
        raise SystemExit(f"La carpeta '{folder}' no existe o no es un directorio.")

    files_payload = collect_files(folder)
    if not files_payload:
        raise SystemExit(f"No se encontraron archivos en la carpeta '{folder}'.")

    print(f"[+] Subiendo {len(files_payload)} archivos de '{folder}' al store:")
    print(f"    {args.store-name}")

    resp = requests.post(
        f"{API_BASE}/upload-files/{args.store-name}",
        files=files_payload,
        timeout=600,  # hasta 10 minutos, por si son archivos pesados
    )

    # IMPORTANTE: cerrar archivos locales
    for _, file_tuple in files_payload:
        file_tuple[1].close()

    print(f"[+] Status: {resp.status_code}")
    try:
        print(resp.json())
    except Exception:
        print(resp.text)


if __name__ == "__main__":
    main()
```

> Ojo: en el script, donde puse `args.store-name` debe ser `args.store_name` (en Python no se aceptan guiones en nombres de variables). Corrijo en el ejemplo de uso abajo.

---

### C√≥mo usarlo para tus 3 categor√≠as

Con el backend corriendo:

```bash
# 1) LEYES (~150 archivos)
python scripts/batch_upload.py \
  --store-name "EL_STORE_NAME_DE_LEYES" \
  --folder "data/leyes"

# 2) TR√ÅMITES (~20 archivos)
python scripts/batch_upload.py \
  --store-name "EL_STORE_NAME_DE_TRAMITES" \
  --folder "data/tramites"

# 3) GENERAL (leyes + tr√°mites)
python scripts/batch_upload.py \
  --store-name "EL_STORE_NAME_DE_GENERAL" \
  --folder "data/leyes"

python scripts/batch_upload.py \
  --store-name "EL_STORE_NAME_DE_GENERAL" \
  --folder "data/tramites"
```

Y listo:

* Tienes **un store solo de leyes**,
* **otro solo de tr√°mites**,
* y **uno general con todo mezclado**.

---

## 4Ô∏è‚É£ C√≥mo se consulta luego (leyes / tr√°mites / general)

Una vez cargado todo, puedes probar en Swagger o con `curl`:

```bash
# Solo LEYES
curl -X POST "http://localhost:8000/query/EL_STORE_NAME_DE_LEYES" \
  -H "Content-Type: application/json" \
  -d '{
        "query": "¬øCu√°l es el l√≠mite m√°ximo de comisiones que pueden cobrar las AFORE?",
        "prompt_profile": "default"
      }'

# Solo TR√ÅMITES
curl -X POST "http://localhost:8000/query/EL_STORE_NAME_DE_TRAMITES" \
  -H "Content-Type: application/json" \
  -d '{
        "query": "¬øQu√© documentos necesito para un retiro parcial por desempleo?",
        "prompt_profile": "default"
      }'

# GENERAL
curl -X POST "http://localhost:8000/query/EL_STORE_NAME_DE_GENERAL" \
  -H "Content-Type: application/json" \
  -d '{
        "query": "Explica c√≥mo funciona el SAR y qu√© tr√°mites b√°sicos existen",
        "prompt_profile": "default"
      }'
```

En el frontend, lo √∫nico que tienes que hacer es:

* Guardar/mapear los 3 `store_name`s.
* Ofrecer un selector:

  * `topic = "leyes"` ‚Üí usa `STORE_LEYES`
  * `topic = "tramites"` ‚Üí usa `STORE_TRAMITES`
  * `topic = "general"` ‚Üí usa `STORE_GENERAL`

y mandar ese `store_name` al endpoint `/query/{store_name}`.

---

Si quieres, en el siguiente mensaje puedo:

* Ajustar cualquier detalle del script (por ejemplo dividir en lotes de 30 archivos).
* O bien hacer una versi√≥n donde en lugar de usar los `store_name` directamente, uses `topic=leyes|tramites|general` y el backend se encargue de traducirlo a IDs leyendo el `.env`.
